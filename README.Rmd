---
output: 
  github_document:
    toc: FALSE
    toc_depth: 3
---

# ðŸ‘‹ Thanks for dropping by!


Stuff I do:

- [x]  Descriptive Statistics, Statistical Modeling, Machine Learning (with emphasis on viz)
- [x]  Visualization (ggplot2, plotnine, Tableau), Table Generation (tidypivot, gt, greattables, flextable)
- [x]  Dashboarding (Experience with Shiny, powerBI, Tableau)
- [x]  Research Reporting (Scholarly, Institutional; Rmarkdown, Quarto)
- [x]  Workflow/Pipeline Management (tidyverse, base R, SQL)
- [x]  Research Software Engineering (experienced R package maintainer, > 5 years)
- [x]  Statistical Consulting
<!-- - [x]  Technical Communication (oral and written) -->
- [x]  Data Science Education (excellence in class, and world wide)
- [x]  Version Control, Collaborative Workflows, Git, Github, Github Actions
- [x]  Productive, Professional, and Friendly Working Relationships with Collaborators with diverse technical and subject area backgrounds
- [x]  Tidyverse Fluency (dplyr, ggplot2, tidyr, purrr, etc)
- [x]  R Packaging Expertise (workflow tools, documentation, testing)
- [x]  Visualization Frameworks (spatial, network, grammar of graphics, ggdims)
- [x]  Contributor to popular open source libraries (ggplot2, plotnine)
- [x]  Author, maintainer of popular libraries (flipbookr, ggcirclepack)
- [x]  Experience with CRAN submission process and maintanance (flipbookr, tidytitanic)

<!-- Bachelor's degree in software development or a directly related field from an accredited institution. -->

<!-- Three (3) years of software development, software or system engineering, bioinformatics, or directly related IT professional experience. -->

<!-- Two (2) years of experience in data analysis using R with common analysis utilities (Tidyverse, dplyr, etc.). -->

<!-- Two (2) years of experience with manipulating large datasets to transform, profile, sanitize, explore, analyze, and present information in a reproducible way. -->

<!-- Two (2) years of experience with version control, preferably with Git -->

<!-- Experience developing and deploying reusable and robust R packages, including familiarity with testing frameworks (testthat) and documentation generation (roxygen2). -->


<!-- A combination of education and related technical/military/paraprofessional experience may be substituted for a bachelorâ€™s degree on a year for year basis. An advanced degree (Masters or Doctorate) may be substituted for experience on a year for year basis if the degree is in a field of study directly related to the work assignment. -->


<!-- - [x]  Reproducible computing environments, containerization -->

```{r, include = F}
library(tidyverse)
knitr::opts_chunk$set(warning = F, message = F, echo = T)

```



I work on tools that make data analytics more fluid and intuitive --- and on tools which expose this fluidity! These tools also allow us to better record 'conversations' with data and workflows.



<!-- ```{r} -->


<!-- ``` -->


<!-- exported functions: `r getNamespaceExports("ggplyr")` -->

<!-- tidypivot - -->

<!-- exported functions: `getNamespaceExports("tidypivot")` -->

<!-- ggdims - -->

<!-- exported functions: `r getNamespaceExports("ggdims")` -->

<!-- ggregions - `r getNamespaceExports("ggregions")` -->





<!-- statexpress - `r getNamespaceExports("statexpress")` -->


<!-- Record conversations: -->

<!-- ggprop.test - `r getNamespaceExports("ggprop.test")` -->



<!-- Expose this fluidity and elegance: -->

<!-- flipbookr - `r getNamespaceExports("flipbookr")` -->

<!-- ggram - -->

<!-- Education: -->

<!-- easy-geom-recipes -->

<!-- easy-geom-recipes-python -->

<!-- easy.geom.recipes.package -->


<!-- knitrExpress -->



My popular projects include [flipbookr](https://github.com/EvaMaeRey/flipbookr) (on CRAN and maintained since 2020, with \>20000 downloads and used across disciplines) and ggplot2 extensions like [ggcalendar](https://evamaerey.github.io/ggcalendar/), [ggcirclepack](https://github.com/EvaMaeRey/ggcirclepack) and others.

Professionally, I've worked in higher education and government where my work involved analytics, teaching, and policy implementation.

I'm interested in lightening cognative load, first, when interpreting data visualizations and, second, when writing and reading code used to build data visualizations. I believe intuitiveness of plot composition tools (coding and gui interfaces) often translates to more effective visualization. With logical, easy-to-use tools, we're better positioned to build compelling, easy-to-interpret visualizations, rather than stopping at 'good enough'. For these reasons, I'm a big fan of the elegant and intuitive grammar of graphics visualization frameworks.

My technical expertise is in ggplot2 and my current focus is on extension and supporting extenders. I co-founded and organize the [ggplot2 extenders club](https://ggplot2-extenders.github.io/ggplot-extension-club/), have created ['easy geom recipes'](https://evamaerey.github.io/easy-geom-recipes/), am writing [ggplot2 extension cookbook](https://github.com/EvaMaeRey/ggplot2-extension-cookbook), and developing ['express' methodologies for extension](https://github.com/EvaMaeRey/ggexpress) (why shouldn't everyone be using extension and why shouldn't we be using them even on an *ad hoc* basis?).

I've studied some of these new educational materials via [survey response and focus groups](https://evamaerey.github.io/easy-geom-recipes/survey_results_summary.html). For more on the motivation for these efforts, see ['everyday ggplot2 extension'](https://evamaerey.github.io/everyday_ggplot2_extension/).

I'm interested in the transformational effects that access to tailored, principled data visualization tools can have on analytic and teaching spaces. I work on greater accessibility for analysts, research, and students to craft tool suited to their particular data challenges.

Previously, I had a greater focus on illuminating the grammar of *base* ggplot2, creating materials like [a ggplot2 grammar guide](https://evamaerey.github.io/ggplot2_grammar_guide/about) and [the ggplot2 flipbook](https://evamaerey.github.io/ggplot_flipbook/ggplot_flipbook_xaringan.html#1).

I am also especially interested in entry points to R package writing, working on resources like a ['companion guide](https://evamaerey.github.io/package_in_20_minutes/package_in_20_minutes) to to Jim Hester's talk how to write and R package in 20 minutes' (2000) I have also looked comparatively at literate package writing tools of [{fusen}](https://thinkr-open.github.io/fusen/), [{litr}](https://jacobbien.github.io/litr-project/), and my own 'readme-to-package' approach, including via an coordinating a virtual meeting with the authors at RLadies Denver, [March 2024 meeting](meetup.com/rladies-denver/events/299879858/?eventOrigin=group_past_events). The [note package (2025)](https://github.com/musician-tools/note) uses my [{knitrExtra}](https://github.com/EvaMaeRey/knitrExtra) package to create the note package (a lightly rewritten version of what Jim Hester presented) from within a README.

Some work projects that are especially motivating:

1. [{ggdims}](https://github.com/EvaMaeRey/ggdims): a framework for dimension reduction in the ggplot2 grammar.  

This project breaks new ground in the ggplot2 ecosystem so that 'aggregation' can be as easily handled across *features* (columns) as it is to aggregate across samples (rows).

```{r, echo = F, message = F, warning=F}
library(tidyverse)
penguins_normalized <- penguins |> 
  remove_missing() |> 
  mutate(bill_len = bill_len/ sd(bill_len) - mean(bill_len)) |>
  mutate(bill_dep = bill_dep/ sd(bill_dep) - mean(bill_dep)) |> 
  mutate(flipper_len = flipper_len/sd(flipper_len) - mean(flipper_len)) |> 
  mutate(body_mass = body_mass/sd(body_mass) - mean(body_mass)) 
```


```{r, fig.height=3}
library(ggplot2)
library(ggdims)

pca <- penguins_normalized |>
  ggplot() + 
  aes(dims = dims(bill_len:body_mass),
      fill = species) +
  geom_pca() 

tsne <- penguins_normalized |>
  ggplot() + 
  aes(dims = dims(bill_len:body_mass),
      fill = species) +
  geom_tsne() 

umap <- penguins_normalized |>
  ggplot() + 
  aes(dims = dims(bill_len:body_mass),
      fill = species) +
  geom_umap() 
  
library(patchwork)
pca + labs(title = "PCA") + 
  tsne + labs(title = "t-SNE") + 
  umap + labs(title = "UMAP") + 
  plot_layout(guides = "collect") + 
  plot_annotation(title = "Dimension reduction techniques with 'Palmer Penguins' data")

```

2. The packaging assistance tool {knitrExtra} that allow R packages to be quickly sketched out inside the context of the narrative that motivates their creation.  This allows for a faster iteration and feedback - ensuring package end-user requirements are met and that API concerns can be addressed early on. 

From a README.Rmd or .qmd file or similar, one can write out a package narrative alongside package code.  Then code can be copied to a file in the appropriate directory to satisfy packaging requirements. 

```{r, eval = F}
knitrExtra::chunk_to_dir("my_function", dir = "R")
```

3. [{ggregions}](https://github.com/EvaMaeRey/ggregions)

The ggregions changes how analysts can interact with [simple features](https://en.wikipedia.org/wiki/Simple_Features) (sf) data.  Specifically it provides utilities that allow package authors to quickly and cleanly create region-aware `geom_*()` functions (smart layers). 

Writing a new region-aware geom with ggregions is simple. First reference data is prepared which contain region identifiers as well as a column (geometry), containing boundary information.  Then a region-aware user-facing layer function can be defined using the ggregions utility. 

```{r}
# 1. prepare reference data
us_states_ref <- usmapdata::us_map() |> 
  select(state_name = full, 
         state_fips = fips, 
         state_abb = abbr, 
         geometry = geom)

# 2. define user-facing `geom_*` function with ggregions utility
geom_state <- ggregions::write_geom_region_locale(ref_data = us_states_ref)
```

Then, user experience is very intuitive. The user can graph geo-referenced data in an intuitive way with smart layers like geom_state() --- border information is merged on under the hood. 

```{r}
# data without boarder info, but which references geographic regions
head(us_rent_income) 

# plot
us_rent_income |> 
  filter(variable == "rent") |> 
  ggplot() + 
  aes(state_name = NAME, 
      fill = estimate) + 
  geom_state() +  
  scale_fill_viridis_c()
```

Notably, this framework may also be used with other atlas types, for example anatomical atlases.


```{r anatogram, child = "../mytidytuesday/2026-01-21-gganatogram-x-ggregions/gganatogram-x-ggregions-public.Rmd"}
```
---

See [{ggswitzerland}](https://github.com/EvaMaeRey/ggswitzerland) for a demonstration package created with the ggregions utility and framework.  ggswitzerland allows users to intuitively and concisely replicate the stunning work of Timo Grossenbacher and Angelo Zehr:

```{r swiss}
library(tidyverse)
library(ggswitzerland)
theme_map() |> theme_set()

head(muni_income_data)

muni_income_data |>
  ggplot() + 
  stamp_mountains() + 
  aes(muni_name = municipality) +
  geom_muni() + 
  aes(fill = mean_quantiles) +
  stamp_canton(fill = "transparent") + 
  stamp_lake(fill = "#D6F1FF")
```


---

4. Flipbookr's new `reveal_live` capabilities

Now, flipbookr can reanimate pipelines with an API similar to the popular package 'reprex', making `reveal_live` available to users.

First, the user should copy some code that they would like 'animated' to their clipboard. 

Then they can execute the code `reveal_live()`, which will create a code-output movie based the code on the clipboard and any objects that are in memory.

---

5. Routines that link physical phenomena with statistical models

[ggsprings](https://github.com/EvaMaeRey/ggsprings) (with Dr. Michael Friendly) helps instructors/students note the commonality between OLS and a spring system, where the spring system will minimize the potential energy of the system and the OLS model is the line that minimizes the sum of the squared residuals.  

```{r springs}
library(ggsprings)

ggplot(anscombe) +
  aes(x2, y2) + 
  geom_point() + 
  geom_smooth(method = lm) + 
  ggsprings:::geom_residual_springs(method = lm)
```


---

See also, [ggprop.test](https://github.com/EvaMaeRey/ggprop.test) which helps educators/students understand the prop test.  The functionality encourages students to notice that the proportion is also the balancing point of the data (TRUE = 1 and FALSE = 0 cases).  

```{r prop.test.prep, echo  = F, message=F, warning=F}
library(tidyverse)

isi_donor_url <- "https://www.isi-stats.com/isi/data/prelim/OrganDonor.txt"

donor <- read_delim(isi_donor_url) %>%
  select(Default, Choice) %>% 
  dplyr::mutate(decision = ifelse(Choice == "donor", "donor (1)", "not (0)")) %>% 
  dplyr::mutate(decision = fct_rev(decision)) 

ggchalkboard:::theme_blackboard(base_size = 14) |> theme_set()
snapshot <- function(...){ggplyr::intercept(...)}
```

```{r prop.test}
head(donor)

library(ggprop.test)
donor |>
  ggplot() + 
  aes(x = decision) +
  geom_stack() + 
  geom_stack_label() + snapshot("p1") +
  geom_support() + snapshot("p2") +
  geom_prop() + 
  geom_prop_label() + snapshot("p3") +
  stamp_prop() + 
  stamp_prop_label() + snapshot("p4") +
  stamp_eq_norm_prop() +
  geom_normal_prop_null() + snapshot("p5") +
  geom_normal_prop_null_sds() + snapshot("p6")
```


If we discuss each of the snapshot points, we could write something like this:

```{r patchproptest, fig.height=11, fig.width = 8, out.width="60%"}


library(patchwork)
(p1 + p2) / 
(p3 + p4) /
(p5 + p6)  + 
  patchwork::plot_annotation(
    tag_levels = 1,
    title = "A {ggprop.test} 'graphical poem' addressing the question: 
    
        For survey of 161 individuals on willingness to serve as 
        organ doners in the case of an accident, is there evidence
        that responses rate differs from a 50/50 split, 
        when 53 individuals respond 'no' and 108 individuals respond 'yes'?
      ",
    subtitle = "1. Draw bar chart (stacks) for each category.
2. Draw theoretical limits for proportion (0 to 1).
3. Calculate proportion and place (also the balancing point for stacks!).
4. Place null (look at the initial question, 'differs from 50/50' in this case)
5. Draw distribution for NULL, where SD = sqrt((p * (1-p))/n)
6. Calc z-score - where does observed prop fit into NULL distributions 
   (i.e. how many standard deviations fit between .5 and .67)"
                             )
```

---

6. Educational work in the ggplot2 extension space, 

Testimonials:

> I was never a "ggextender" myself until [going through the recipes]! ... Easy geom recipes [are] a series of tutorials on creating ggplot2 extensions. Following "recipes", you methodically create three extensions. Each time, certain key knowledge points are reinforced and new variations are introduced. 'R Works' Isabella VelazquÃ©s

> Just wanted to share that I'm spending some of my weekend reading your brilliant resource here. This is so well written and delightful as a resource; wish I had had it years ago and thrilled to know it now. Thank you for writing! - Emily Riederer

> The format works for me. I've read tutorials about creating extensions before, but this one made it look easier and more intuitive. I want to try again writing my own! - Georgios Karamanis

```{r, eval = F, echo = F}
file.copy("../posit-consulting/report2_easy_geom_recipes.pdf", "report2_easy_geom_recipes.pdf")
```

See the [report](https://evamaerey.github.io/evamaerey/report2_easy_geom_recipes.pdf) to Posit evaluating the resources based on a survey and focus group. Feedback was collected was from two software engineers; four university level Statistics and Data Science educators, a Data Visualization Professional; and a data science product professional. 

---


