---
title: Selection effects
author: Gina Reynolds
date: '2018-06-25'
slug: selection-effects
categories: []
tags: []
header:
  caption: ''
  image: ''
---

Saw this tweet by @Nolan_Mc, and thought, this is good fodder for a quick blog post -- a true one-hour one-off.  

```{r echo=FALSE}
blogdown::shortcode('tweet', '1008498114037248000')
```

My limited goals: 

- translate to R code
- visualize

Perhaps the central difference between working in the Stata environment and in R is that in R you always have to be declaring which data frame you are working with.  In Stata, you just have one active data frame and then you can refer to the variables by their names alone.  The tidyverse tools with piping make working in R feel more like working in Stata in my experience.   

Okay so here again is the code in Stata:  

- set obs 20000
- gen x = runiform()
- gen y = x + .1*rnormal()
- corr x y
- corr x y if x > .95

And here is the translation to R (just using base R): 

```{r}
set.seed(39475)
x <- runif(20000)
y <- x + .1*rnorm(20000)
cor.test(x, y)
cor.test(x[x > .95], y[x > .95])
```

It is interesting to note that the strength of the relationship (the correlation coefficient) is estimated to be much lower in the case where we have subset to only the top x values.  That's the whole point of the twitter post.

Now, I've only actually used base R up to this point.  For the visualization, we will use ggplot2, which works best if you have your data in a data frame.  Let's load the tidyverse tools (ggplot2, dplyr, etc.), make that data frame, and visualize!  I also compute the model that is overlaid on the scatterplot. 

```{r, warning = F, message = F}
library(tidyverse)
df <- data_frame(x,y)
```

Now we can use ggplot() to explore the simulated data.  First the full data:

```{r}
ggplot(df) +
  aes(x = x, y = y) + 
  geom_point(alpha = .2) +
  theme_bw() + geom_smooth(method = lm)
```
And fitting a linear model:

```{r}
summary(lm(y ~ x, data = df))
```

In this model the R squared (variance explained) is high -- .89.  The variable x does seems to do a good job of predicting y. 

Now plotting the subset:

```{r}
ggplot(df %>% filter(x > .95)) +
  aes(x = x, y = y) + 
  geom_point(data = df, col = "grey") +
  geom_point() +
  theme_bw() + geom_smooth(method = lm)
```
or:

```{r}
ggplot(df %>% filter(x > .95)) +
  aes(x = x, y = y) + 
  geom_point() +
  theme_bw() + geom_smooth(method = lm)
```


```{r}
summary(lm(y ~ x, data = df %>% filter(x > .95)))
```

Looking at this data, x seems to explain very little. The variance explained (R^2) is very small. The variable x seems not to do such a good job of explaining y given the subset of data. 

This exercise reminds us that the R2, or variance explained, in bivariate linear regression is equivalent to the Pearson Correlation Coefficient squared.

```{r}
cor(x,y)^2
summary(lm(y ~ x, data = df))$r.squared
```

Which makes me want to link to this wonderful gif from assessingpsyche.files.wordpress.com:  

![](https://assessingpsyche.files.wordpress.com/2014/07/varianceexplained.gif)

Found at: 

https://assessingpsyche.wordpress.com/2014/07/10/two-visualizations-for-explaining-variance-explained/

Did I spend only an hour with this post?  No, sadly, but not too too too much more!

# Now let's montecarlo

Not part of original goals! Why am I still updating this post!   

Let's think about what we might have observed upon multiple realizations of this experiment.

```{r}
estimate <- c()
case <- c()

for(i in 1:2000){
x <- runif(20000)
y <- x + .1*rnorm(20000)
if(i<=1000){
estimate[i] <- summary(lm(y~x))$coefficients[2,1]
case[i] <- "full"
}else{
estimate[i] <- summary(lm(y[.95 < x] ~ x[.95 < x]))$coefficients[2,1]
case[i] <- "selection"
}
}

df <- data_frame(estimate, case)

ggplot(df) +
  aes(x = estimate, fill = case) +
  geom_density(alpha =.2) +
  geom_rug() +
  facet_wrap(~case, scales = "free")


```

Look at how the distributions differ.  The estimates that we might observe very narrow in the case of observing all the data, and rather wide in the case where we observe the subset of data. 


# Selection *bias* 

Above we see how looking at a subset of our data, just a small range of our independent variable, results in inefficiency.  The variance explained will likely be less. 

Selection on *dependent variable* can be hazardus as it introduces bias.  Here is an example of that case with a similar set up.  Let's just start with the montecarlo:

```{r}
estimate <- c()
case <- c()

for(i in 1:2000){
x <- runif(20000)
y <- x + .1*rnorm(20000)
if(i<=1000){
estimate[i] <- summary(lm(y~x))$coefficients[2,1]
case[i] <- "full"
}else{
estimate[i] <- summary(lm(y[.5 < y] ~ x[.5 < y]))$coefficients[2,1]
case[i] <- "selection"
}
}

df <- data_frame(estimate, case)

ggplot(df) +
  aes(x = estimate, fill = case) +
  geom_density(alpha =.2) +
  geom_rug() +
  facet_wrap(~case, scales = "free")
```

## plotting one of these simulations


```{r}
df <- data_frame(x, y)
ggplot(df %>% filter(y > .5)) +
  aes(x,y) + 
  geom_point(data = df, col = "grey") +
  geom_point() + 
  geom_smooth(method = lm)
```

You can see how the linear model underestimates the trend. 