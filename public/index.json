[{"authors":["admin"],"categories":null,"content":"Welcome. I am a Visiting Teaching Assistant Professor at the University of Denver\u0026rsquo;s Josef Korbel School of International Studies. I\u0026rsquo;ve previously lectured at the Technische Universität Dresden\u0026rsquo;s Center for International Studies and have consulted on a project with the Violence Prevention Research Program at the University of California Davis. My PhD is from the University of Illinois, where my dissertation won 2018 Burkholder Award for Best Dissertation in the Political Science department of the University of Illinois.\nI study effectiveness of international institutions and law especially in the area of security. More broadly, my areas of study have been International Relations, Methodology, and Comparative Politics with an emphasis on Latin American Politics. My dissertation focused on compliance with supranational law, using UN Security Council resolutions as my principle case; I conducted dissertation field research in Brazil in the Federal House of Deputies.\nI have extensive experience in data analysis and teaching data science. I worked as a statistics consultant, at the Applied Technologies of the Arts and Science (ATLAS), at the University of Illinois from 2013-2015. I also served \u0026ldquo;Methods TA\u0026rdquo; in Political Science in the 2015-2016 academic year at the University of Illinois, providing assistance and expertise both to undergraduate and graduate students. At TU Dresden, in 2018, I designed and taught a course introducing students to data science tools and statistical analysis for political research. I primarily teach methodology and data science courses at the Korbel School. Research interests in methodology include communicating uncertainty, visual exposition of statistical concepts and electoral rules.\nI have been awarded the Fulbright Fellowship (Argentina 2008), Foreign Language and Area Studies Fellowships (2009-2011), Nelle Signor Travel Fellowship (Brazil 2011) and have participated in specialized workshops including the Empirical Implications of Theoretical Models (2010), Public Policy and Nuclear Threats (2013), the Berkeley Institute for Transparency in the Social Sciences (2015) workshops, the Zurich Summer School for Women in Political Methodology (2017), and the Lorentz Workshop: Empirical Research on International Organizations (2018).\nPreviously to my academic career, I\u0026rsquo;ve worked at the U.S. Department of Commerce\u0026rsquo;s Bureau of Industry and Security in chemical and biological export controls and have worked in lithium-ion battery failure diagnostics at the Chemical Engineering Division of Argonne National Laboratory.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Welcome. I am a Visiting Teaching Assistant Professor at the University of Denver\u0026rsquo;s Josef Korbel School of International Studies. I\u0026rsquo;ve previously lectured at the Technische Universität Dresden\u0026rsquo;s Center for International Studies and have consulted on a project with the Violence Prevention Research Program at the University of California Davis. My PhD is from the University of Illinois, where my dissertation won 2018 Burkholder Award for Best Dissertation in the Political Science department of the University of Illinois.","tags":null,"title":"Evangeline Reynolds","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":"","date":1570622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570622400,"objectID":"740c28452b8004ad3f4eb03ddefd15c6","permalink":"/talk/korbel-reproducibility/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/korbel-reproducibility/","section":"talk","summary":"What are project organization best practices? Discussion will emphasize on dynamic documents with the aim of getting participants up and running with these tools.","tags":["reproduciblility","workflow","R"],"title":"Dynamic Documents and Reproducibility","type":"talk"},{"authors":[],"categories":null,"content":"","date":1563454800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563454800,"objectID":"a8f46b0e3a127d87c5f1bcfcfee8f59b","permalink":"/talk/ggplotgrammar/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/ggplotgrammar/","section":"talk","summary":"Explaining the grammar of graphics visualization philosophy and using ggplot2, its implementation in R","tags":[],"title":"A ggplot2 grammar guide","type":"talk"},{"authors":[],"categories":null,"content":"","date":1560162600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560162600,"objectID":"e538450147a0da557fd35c18c53ba35e","permalink":"/talk/illinois-reproducibility/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/illinois-reproducibility/","section":"talk","summary":"Building a framework for a transparent and reproducible workflow within the R/RStudio ecosystem","tags":["reproduciblility","workflow","R"],"title":"The three Rs: R, Rmarkdown, and Reproducibility","type":"talk"},{"authors":[],"categories":null,"content":"","date":1557403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557403200,"objectID":"9971657887c0b5231882317be10b24ad","permalink":"/talk/shallow-fakes/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/shallow-fakes/","section":"talk","summary":"Assessing the potential for spoofing and faking in data visualization and what to do about it","tags":["Data Visualization","Journalism","Social Media"],"title":"Shallow Fakes","type":"talk"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["data visualization","teaching"],"content":" This post has lots in common with previous posts on “the layered presentation of graphics”. It is about building up plots, but now with a focus on this incremental change for teaching ggplot2. The rational is that observing the cause and effect of incremental change is easier to digest, and that the repetition in this approach means students have more chances to learn the ggplot2 functions.\nMy recent tweet presented the technique:\nHere, building up a #ggplot2 as slowly as possible, #rstats. Incremental adjustments. #rstatsteachingideas pic.twitter.com/nUulQl8bPh\n\u0026mdash; Gina Reynolds (@EvaMaeRey) August 13, 2018  People reacted positively. Now here is the comparison between a traditional ggplot construction and the more verbose, slow ggplot construction.\nlibrary(tidyverse) library(gapminder) df_2007 \u0026lt;- gapminder %\u0026gt;% filter(year == 2007) Traditional approach:\nggplot(data = df_2007, mapping = aes(x = gdpPercap, y = lifeExp, col = continent)) + geom_point() + labs(title = \u0026quot;Wealth and life expectancy in 2007\u0026quot;, x = \u0026quot;GDP per capita (inflation adjusted)\u0026quot;, y = \u0026quot;Life Expectancy\u0026quot;, col = \u0026quot;\u0026quot;) + theme_bw() Verbose, slow approach:\nggplot(data = df_2007) + aes(x = gdpPercap) + aes(y = lifeExp) + geom_point() + aes(col = continent) + labs(title = \u0026quot;Wealth and life expectancy in 2007\u0026quot;) + labs(x = \u0026quot;GDP per capita (inflation adjusted)\u0026quot;) + labs(y = \u0026quot;Life Expectancy\u0026quot;) + labs(col = \u0026quot;\u0026quot;) + theme_bw() Same result, but students are perhaps more likely to go slow, and mentally make connections between adjustment to product and the functions for making those adjustments, given that each change gets its own line of code and that there is repetition built into this exercise. After using this approach to introduce students to ggplot, students can be alerted to the fact that they can gather up arguments into one function.\nDownsides? A possible downside is that most teaching materials don’t take this approach. Perhaps it will be confusing to have the two approaches in students’ head. This is an unanswered empirical question, but my hunch is that newbies won’t find this jump so hard to make.\n ","date":1536796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536796800,"objectID":"76d674782e24fe0fdfdcf28afc3c84d5","permalink":"/post/slow-ggplot/","publishdate":"2018-09-13T00:00:00Z","relpermalink":"/post/slow-ggplot/","section":"post","summary":"This post has lots in common with previous posts on “the layered presentation of graphics”. It is about building up plots, but now with a focus on this incremental change for teaching ggplot2. The rational is that observing the cause and effect of incremental change is easier to digest, and that the repetition in this approach means students have more chances to learn the ggplot2 functions.\nMy recent tweet presented the technique:","tags":["ggplot2","data visualization"],"title":"Slow ggplot","type":"post"},{"authors":null,"categories":[],"content":" So this wasn’t on today’s to-do list, but there seems to be a cash prize associated with this rabbit hole due to this tweet:\nI generally fall in the camp of people who are skeptical of graduate-level journalism schools, but I would absolutely pay Columbia University $98,000 if it could teach me how to clearly and concisely translate “one standard deviation from the mean” for regular readers — Max Fisher (@Max_Fisher) August 13, 2018 Note: Tweet has been deleted; the text is from my blogpost when the tweet was still active. Updating Sept 18, 2018\nAnd communicating about standard deviations. That’s like one of my favorite topics! The project is now on my to-do list.\nFull disclosure, I’ve already written on this topic here: From N to Standard Deviation: Visualizing Univariate Statistics. The current post was be a matter of focusing on variance and sd, and clean-ups as any messes were encountered (encountered they were!). One more thing: the calculation is for population variance and sd.\nSo here we go. I’m using the gapminder dataset which is ever-so-handy as it’s available in an R package (thanks Jenny Bryan). For the exercise I’ll just be looking at European countries in 2007, and focusing exclusively on the life expectancy variable.\nLet’s look at a plot of the data.\nCool. So what are the variance and standard deviation for life expectancy of European Countries in 2007?\nWell, the equation for variance is as follows.\n\\(\\sigma^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}\\)\nCan you mentally visualize it? Maybe not? Let’s walk through together then.\nFirst we need the mean, \\(\\mu\\). We plot it in red on the figure.\nThen we do an operation for each of our observations, \\((x_i - \\mu)^2\\). In words, for an observation, say Romania, take the difference for its life expectancy value from the mean. Then square that. So, let’s just do that for one of our observations. Our Romania example will work fine. We plot the point representing Romania in green.\nNow take the difference between the mean for all observations, \\(\\mu\\), and the life expectancy for Romania.\nCalculation for Romania: \\((x_{Romania} - \\mu)\\) This difference is shown with the green line:\nVariance: \\(\\sigma^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}\\)\nThen we need to square that difference. This resultant value is equal to the area of a square that has as one of its sides the difference in life expectancy for Romania and the mean life expectancy for all observations. It is a transparent green square in the plot.\nCalculation for Romania: \\((x_{Romania} - \\mu)^2\\) Variance: \\(\\sigma^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}\\)\nNow we need to do the same for all of our observations. Not just Romania, but also Germany, Italy, Sweden, France, and so on.\nDoing that we get the result (Yup, Romania’s still there in green):\nThe average of all of these areas is the variance. We simply sum up all of the resulting areas that are shown in the plot (note that these areas are overlapping), and divide by the number of these squares:\n\\(\\sum_{i=1}^{n}(x_i - \\mu)^2/n\\)\nThe result is the variance. It is equivalent to the area of the orange square in the plot below. Again, it is simply the average area of all of the squares we created previously. The corner of the orange square, whose area is the variance, happens to be at the mean value, but it need not be so.\nStandard Deviation: Now, calculating the standard deviation is straightforward. Take the square root of the variance (that orange square).\nStandard Deviation: \\(\\sigma = \\sqrt\\frac{\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}\\)\nThe standard deviation on the plot can be represented as simply the length of the edge of the square whose area is the variance (i.e. the length of the side of the bright orange square). The standard deviation (the resulting length) is highlighted in blue on the plot:\nThat’s it! I hope that this will help you visualize variance and sd in the future.\nDiscussion question(s):\nWhat do you think of as the first step to finding the variance and standard deviation? Which measure do you think feels more useful as a description of spread of a variable? What are the units of each?\nQuick final note: We could equivalently represent the squared differences (observation value - mean squared) in a bar chart:\nAnd then we divide through by the number of observations (e.g. the total number of squares) which gives us our aim - the variance, represented as \\(\\sigma^2\\):\n\\(\\sigma^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}\\)\nAnd taking the average gives us the level of the dotted line and grey bars:\nSo that is the variance: the mean of the areas of the squares that have edges that are the distance between observation values and the population mean value.\nThe standard deviation is is the length of the edge of the square that has the area which is the variance.\nAnd these two last sentences are the reason I like to keep things step-by-step and visual!\n  ","date":1534291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534291200,"objectID":"fe4732c28dea24fafa159fabef458e74","permalink":"/post/variance-and-sd-visualization/","publishdate":"2018-08-15T00:00:00Z","relpermalink":"/post/variance-and-sd-visualization/","section":"post","summary":"So this wasn’t on today’s to-do list, but there seems to be a cash prize associated with this rabbit hole due to this tweet:\nI generally fall in the camp of people who are skeptical of graduate-level journalism schools, but I would absolutely pay Columbia University $98,000 if it could teach me how to clearly and concisely translate “one standard deviation from the mean” for regular readers — Max Fisher (@Max_Fisher) August 13, 2018 Note: Tweet has been deleted; the text is from my blogpost when the tweet was still active.","tags":[],"title":"Visualizing Variance and Standard Deviation","type":"post"},{"authors":null,"categories":[],"content":" Here is my second post is about how to implement a layered presentation of a graphics. My previous implementation used the alpha transparency aesthetic to hide all but one point. But, now, rethink things, now for the 3rd time or so, I just subset the data associated with the first geom layer, leaving the global data complete.\nI think it is more straight forward than messing around with alpha. Several folks brought up geom_blank() having looked at the previous implementation, but I didn’t find it necessary in this case if you are using last_plot() which I think it makes sense to do in this context. Still, geom_blank is good to know about.\nThis time around, I’ll do a little with labeling too. Before, I left labels as variable names, which wouldn’t really be acceptable in a presentation setting, at least with the present example.\nHere again is the original inspiration:\nMy best tip on how to give better quantitative presentations is to (a) use more plots and (b) build up your plots on multiple overlays, as in:\n- Just x-axis (explain it)\n- Add y-axis (explain it)\n- Add 1 data point (explain it)\n- Plot the rest of the data (explain it)\n\u0026mdash; Matt Blackwell (@matt_blackwell) April 30, 2018  And the goal is simply to write an implementation, with some data:\nlibrary(tidyverse) library(gapminder) my_data = gapminder %\u0026gt;% filter(continent == \u0026quot;Americas\u0026quot; \u0026amp; year == 2002) Okay, the data is ready and packages loaded. Now for plotting:\nStep 1: Just x-axis (explain it) ggplot(my_data) + # declare the data you want to use aes(x = gdpPercap) + # declare the aesthetic mapping for x theme_bw(base_size = 20) + # we want label sizes to be a bit bigger labs(x = \u0026quot;Per capita GDP\\n(US$, inflation-adjusted)\u0026quot;)   # instead of variable name, we give x axis a nice name  Step 2: Just y-axis (explain it) last_plot() + # we just add to the last plot aes(y = lifeExp) + # add the aesthetic mapping for y labs(y = \u0026quot;Life Expectancy\\n(years)\u0026quot;) # a nice axis label  Step 3: Add 1 data point (explain it) last_plot() + geom_point(data = my_data %\u0026gt;% filter(country == \u0026quot;Chile\u0026quot;)) # declare the geom to add  I use geom_point() to add the layer where Chile’s data is shown. I indicate that I want points to be added for onlly a subset of the data by specifying that with the data argument.\n Step 4: Plot the rest of the data (explain it) Add another geom_point() layer. This time, the data that will be used is the global data, as there is no data specified in the function.\nlast_plot() + geom_point() Done!\nWhy didn’t I come up with this before? I think it didn’t have the presence of mind because my mind had just been blown by the possibility of adding global aesthetics outside of the ggplot() statement. I was very excited about that discovery! Also, ggplot2 has also had an update since the original implmentation. Perhaps the behavior was different in a previous version. Don’t know. Not gonna check at this point. I guess probably it wasn’t.\n ","date":1532217600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532217600,"objectID":"31f2a3b0e0585004dbcbed442d60711c","permalink":"/post/layered-presentation-of-graphics-take-2/","publishdate":"2018-07-22T00:00:00Z","relpermalink":"/post/layered-presentation-of-graphics-take-2/","section":"post","summary":"Here is my second post is about how to implement a layered presentation of a graphics. My previous implementation used the alpha transparency aesthetic to hide all but one point. But, now, rethink things, now for the 3rd time or so, I just subset the data associated with the first geom layer, leaving the global data complete.\nI think it is more straight forward than messing around with alpha. Several folks brought up geom_blank() having looked at the previous implementation, but I didn’t find it necessary in this case if you are using last_plot() which I think it makes sense to do in this context.","tags":[],"title":"Layered Presentation of Graphics, revised","type":"post"},{"authors":null,"categories":[],"content":" Where should you declare aesthetics? Globally or in the geom_*() function? The answer to this question, in some sense is personal preference, because there are simply different ways to get the same job done in the ggplot architecture. My preference is declaring all aesthetic mappings as global unless there are conflicts.\nBelow is an example that, I hope, will persuade you to my preference. We graph the increase in life expectancy by year for three countries. First, subset some data from the gapminder dataset and we create a scatterplot.\nlibrary(tidyverse) df \u0026lt;- gapminder::gapminder %\u0026gt;% filter(country %in% c(\u0026quot;Germany\u0026quot;, \u0026quot;United States\u0026quot;, \u0026quot;Italy\u0026quot;)) ggplot(df) + aes(x = year, y = lifeExp) + geom_point()  Then we might want to add a line connecting the three countries data year by year. We add geom_line, but notice that grouping needs to be declared to distinguish the countries:\nggplot(df) + aes(x = year, y = lifeExp) + geom_point() + geom_line() Using the grouping, (notice here I’m declaring the grouping as a global aesthetic mapping), the plot makes a bit more sense.\nggplot(df) + aes(x = year, y = lifeExp, group = country) + geom_point() + geom_line() Then, given this declaration, I can add a linear trend line with geom_smooth() for each country.\nggplot(df) + aes(x = year, y = lifeExp, group = country) + geom_point() + geom_line() + geom_smooth(method = lm) This is where I think the gain is. You don’t have to make the grouping declaration twice. (And you can override the global grouping aesthetic, should you so desire, in the geom_smooth() The alternative is redundant:\nggplot(df) + aes(x = year, y = lifeExp) + geom_point() + geom_line(aes(group = country)) + geom_smooth(aes(group = country), method = lm) In conclusion, trust your aesthetic mapping decisions, and just make them global. You might want to use them across more geoms as you build up your plot, and probably you will want consistent aesthetic mapping choices across your geoms. When a conflict arises, then you can simply override the aesthetic as you need to within the specific geom function, as done below.\nggplot(df) + aes(x = year, y = lifeExp, group = country) + geom_point() + geom_line() + geom_smooth(aes(group = NULL), method = lm) ","date":1531699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531699200,"objectID":"79b69d32313d748b383c0a947382b37d","permalink":"/post/mapping-aesthetics/","publishdate":"2018-07-16T00:00:00Z","relpermalink":"/post/mapping-aesthetics/","section":"post","summary":"Where should you declare aesthetics? Globally or in the geom_*() function? The answer to this question, in some sense is personal preference, because there are simply different ways to get the same job done in the ggplot architecture. My preference is declaring all aesthetic mappings as global unless there are conflicts.\nBelow is an example that, I hope, will persuade you to my preference. We graph the increase in life expectancy by year for three countries.","tags":[],"title":"Where should you declare aesthetics?  Globally, or geom-by-geom?","type":"post"},{"authors":null,"categories":[],"content":" A wide data storage format is an efficient and compact way to store information. And this organization perhaps it makes data easier to inspect. We have wide monitors our laptops and destops. However, for visualization and analysis you generally need to transform this data from the wide format to a “tidy”, long format.\nWe look at the case where just one variable is stored in a spreadsheet.\nlibrary(tidyverse) Suppose you have a data frame of rankings of schools by year, and the initial data set is organized as follows (I just build one with tribble()):\ndf_wide \u0026lt;- tribble(~rankings_of_schools_by_year, ~`2000`, ~`2001`, ~`2002`, \u0026quot;U of Illinois\u0026quot;, 1, 2, 3, \u0026quot;TU Dresden\u0026quot;, 2, 3, 1, \u0026quot;U of Denver\u0026quot;, 3, 1, 1, \u0026quot;Hogwarts\u0026quot;, 4,4,4) df_wide ## # A tibble: 4 x 4 ## rankings_of_schools_by_year `2000` `2001` `2002` ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 U of Illinois 1 2 3 ## 2 TU Dresden 2 3 1 ## 3 U of Denver 3 1 1 ## 4 Hogwarts 4 4 4 Restructuring the data with gather, I define the names of columns that will contain the information in the column names (year) and the variable of interest (rank).\ndf_long \u0026lt;- df_wide %\u0026gt;% gather(key = year, value = rank, `2000`:`2002`) df_long ## # A tibble: 12 x 3 ## rankings_of_schools_by_year year rank ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 U of Illinois 2000 1 ## 2 TU Dresden 2000 2 ## 3 U of Denver 2000 3 ## 4 Hogwarts 2000 4 ## 5 U of Illinois 2001 2 ## 6 TU Dresden 2001 3 ## 7 U of Denver 2001 1 ## 8 Hogwarts 2001 4 ## 9 U of Illinois 2002 3 ## 10 TU Dresden 2002 1 ## 11 U of Denver 2002 1 ## 12 Hogwarts 2002 4 Pretty good! But we are not all the way there. Let’s use the code above a base. We need to change the first column name to be more appropriate. Also, the years are encoded as a character variable whereas they should be numeric (in this case integers, as the years are round numbers).\ndf_long \u0026lt;- df_wide %\u0026gt;% gather(key = year, value = rank, `2000`:`2002`) %\u0026gt;% rename(school = rankings_of_schools_by_year) %\u0026gt;% mutate(year = as.integer(year)) df_long ## # A tibble: 12 x 3 ## school year rank ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 U of Illinois 2000 1 ## 2 TU Dresden 2000 2 ## 3 U of Denver 2000 3 ## 4 Hogwarts 2000 4 ## 5 U of Illinois 2001 2 ## 6 TU Dresden 2001 3 ## 7 U of Denver 2001 1 ## 8 Hogwarts 2001 4 ## 9 U of Illinois 2002 3 ## 10 TU Dresden 2002 1 ## 11 U of Denver 2002 1 ## 12 Hogwarts 2002 4 Note to students: Then you might filter by year: filter(year \u0026gt; 2000).\n","date":1530835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530835200,"objectID":"9840e1ba9ea5aa6dc2ead47dd8f2b734","permalink":"/post/wide-to-long-using-the-tidy-verse/","publishdate":"2018-07-06T00:00:00Z","relpermalink":"/post/wide-to-long-using-the-tidy-verse/","section":"post","summary":"A wide data storage format is an efficient and compact way to store information. And this organization perhaps it makes data easier to inspect. We have wide monitors our laptops and destops. However, for visualization and analysis you generally need to transform this data from the wide format to a “tidy”, long format.\nWe look at the case where just one variable is stored in a spreadsheet.\nlibrary(tidyverse) Suppose you have a data frame of rankings of schools by year, and the initial data set is organized as follows (I just build one with tribble()):","tags":[],"title":"Wide data to long using the tidyverse (tidyr's gather function)","type":"post"},{"authors":null,"categories":[],"content":" @drob has posted code to play with on Twitter today. To illustrate what he calls a veridical paradox he’s posted the set up, the code and result of a coin flipping experiment:\nThere are some good and exact explanations in the thread, for this at-first-glance puzzle. But I didn’t see a visualization that might give you quick intuition about what is going on.\nSo I prepare one here. We’ll use the tidyverse packages and stringr.\nlibrary(tidyverse) library(stringr) First we simulate one flip’s possible outcomes.\none_flip \u0026lt;- tribble( ~flip, \u0026quot;Heads\u0026quot;, \u0026quot;Tails\u0026quot; ) We can also simulate the possible outcomes for histories which have equal probability.\ntwo_flips \u0026lt;- crossing(one_flip, one_flip) two_flips And so on…\ncrossing(two_flips, one_flip) For more flips I use a for-loop. Here I just have the histories for six flips. This give us 2^5 (32) equally probable histories. This is enough, I think, to make a viz that might illuminate the paradox.\nflip_histories \u0026lt;- one_flip for(i in 1:4){ flip_histories \u0026lt;- crossing(flip_histories, one_flip) } dim(flip_histories) Now let’s plot these histories to give us some insights about the apparent paradox.\nWe’ll use ggplot2(), so first we get the data into tidy form.\nnames(flip_histories) \u0026lt;- paste0(\u0026quot;flip\u0026quot;, 1:ncol(flip_histories)) flip_histories \u0026lt;- flip_histories %\u0026gt;% mutate(history = 1:n()) tidy_df = gather(flip_histories, \u0026quot;flip\u0026quot;, \u0026quot;outcome\u0026quot;, 1:5) %\u0026gt;% mutate(flip = as.numeric(str_extract(flip, \u0026quot;\\\\d\u0026quot;))) %\u0026gt;% arrange(history) Then we compute the position where we have observed the first heads heads pattern, and where we observed the first heads tails pattern.\ntidy_df = tidy_df %\u0026gt;% group_by(history) %\u0026gt;% mutate(next_flip_outcome = lead(outcome)) %\u0026gt;% mutate(hh = outcome == \u0026quot;Heads\u0026quot; \u0026amp; next_flip_outcome == \u0026quot;Heads\u0026quot;) %\u0026gt;% mutate(first_hh = min(flip[hh], na.rm = T) + 1) %\u0026gt;% mutate(ht = outcome == \u0026quot;Heads\u0026quot; \u0026amp; next_flip_outcome == \u0026quot;Tails\u0026quot;) %\u0026gt;% mutate(first_ht = min(flip[ht], na.rm = T) + 1) Now we can plot the two scenarios of interest side-by-side. The full hypothetical histories are plotted, but the transparency is increased if the goal is reached previously in the flipping space.\nlibrary(cowplot) g_hh \u0026lt;- ggplot(tidy_df) + aes(flip, history, alpha = flip \u0026lt;= first_hh, col = outcome) + geom_point() + scale_alpha_discrete(range = c(.3,1)) + theme_classic() + labs(title = \u0026quot;Heads-heads as success case\u0026quot;) + geom_hline(yintercept = seq(.5,32.5, by = 1), lty = \u0026quot;dotted\u0026quot;, col = \u0026quot;grey\u0026quot;) g_ht \u0026lt;- g_hh + aes(alpha = flip \u0026lt;= first_ht) + labs(title = \u0026quot;Heads-tails as success case\u0026quot;) cowplot::plot_grid(g_hh, g_ht) You observe that the success in flips 1 and 2 for the heads-heads desired outcome leads less opportunities for flips 2 and 3 to be a success, compared with the heads-tails case. In the heads-heads case, success is the kind of success that quenches more success.\nIt might be fun too plot these as branching might-have-been networks (like with tidygraph!). But I will leave that for someone else or another day.\nPackages used: H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.\nHadley Wickham, Romain François, Lionel Henry and Kirill Müller (2018). dplyr: A Grammar of Data Manipulation. R package version 0.7.5. https://CRAN.R-project.org/package=dplyr\nHadley Wickham and Lionel Henry (2018). tidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions. R package version 0.8.1. https://CRAN.R-project.org/package=tidyr\nClaus O. Wilke (2017). cowplot: Streamlined Plot Theme and Plot Annotations for ‘ggplot2’. R package version 0.9.2. https://CRAN.R-project.org/package=cowplot\nHadley Wickham (2018). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.3.1. https://CRAN.R-project.org/package=stringr\n ","date":1529366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529366400,"objectID":"5a5faedb22ab8ce7b21c8eeeafd962f6","permalink":"/post/sequences-probabilities/","publishdate":"2018-06-19T00:00:00Z","relpermalink":"/post/sequences-probabilities/","section":"post","summary":"@drob has posted code to play with on Twitter today. To illustrate what he calls a veridical paradox he’s posted the set up, the code and result of a coin flipping experiment:\nThere are some good and exact explanations in the thread, for this at-first-glance puzzle. But I didn’t see a visualization that might give you quick intuition about what is going on.\nSo I prepare one here. We’ll use the tidyverse packages and stringr.","tags":[],"title":"The visual taming of a paradox","type":"post"},{"authors":null,"categories":[],"content":" Every couple of weeks I like to explore data that’s brand new to me. I anticipate a one-hour, one-off project. Usually this turns out to be a beautiful lie, and the projects chew up much more time. Still, this enticing time-line is pulling me into new projects from time to time.\nEarlier this week, I heard about the dispute of authorship of some of the Federalist papers.\n\u0026quot;Hamilton wrote the other 51!\u0026quot;, go the unqualified song lyrics. But did he? 😮 Federalist papers authorship inquiries sound like a great way to introduce #textasdata ideas, especially to Hamilton the Musical fans. Musings upon read of @arthur_spirling and @ptrckprry\u0026#39;s \u0026quot;boring\u0026quot;.👍 https://t.co/nm4AoVeOTU\n\u0026mdash; Gina Reynolds (@EvaMaeRey) June 13, 2018  I mentally flagged the topic for one a one-hour, one-off. A who-done-it of text analysis, that conjures up the songs of the biggest Broadway hit since … take your pick. What’s not to like?\nAlso, I’ve been working on a visualization procedure for small corpora like this one; this would be another chance to try it out. The procedure visualizes documents as nodes in a network, and connects the documents with edges only if for the pair, their similarity scores are in the top percentiles for one or the other of the pair members.\nI don’t see other researchers doing this which makes me worry that: 1) there are principled reasons not to do this or 2) I’m not googling hard enough. Regarding the former, in creating such a graph, there is some arbitrary decision-work to be done: what is the similarity score to be used; what threshold of should be used for drawing connections; and which network layout should be draw? A critic might think that, given these decisions, this type of visual summary of a corpus in the best case is impressionistic, and in the worst case invites misinterpretation. The later can be managed, I think, if there is enough time/space to talk/write about the decisions made in creating the graph.\nPutting aside these concerns, I can put together this visualization contentedly.\nThe deciding factor for moving ahead with this quick project was that the Federalist Papers were ready-to-use in corpus in the R package “corpus”, by Patrick O. Perry. Sold! Install and load package.\nlibrary(corpus)  I was also “sold” by the size of the corpus. The Federalist Paper, with its 85 documents lends to my small-corpus network graph visualization. Network graphs tend to get messy and overwhelming looking with more than 100 nodes. Developing this visualization technique, I’ve used a few corpora that have well beyond 100 documents, like UN Security Council Resolutions (in an ongoing project with @felixhass and @F_Bethke, \u0026gt; 2000 documents); Research \u0026amp; Politics Articles (solo, ~ 200 documents); and National Anthems (in ongoing work with @marinkobobic and @Jarek_Kantor, ~ 200 documents), and the options with these way-too-large and too-large corpora is to subsample and plot, or to just leave things messy. But the trim Federalist Papers doesn’t present this problem!\nThe question My question was whether this visualization method could provide insight into who wrote the disputed essays. Apparently Hamilton claimed quite a few more essays, that later Madison claimed that he wrote. Lin Manuel Miranda’s lyrics are consistent with conservative 51 essays for Hamilton. Of course there is a lot of fantastic work already done on this, but I just ignore this fact for the sake of the exercise. The federalist dataset in the corpus package provides authors if known; but disputed authorship is designated NA.\ntable(federalist$author, useNA = \u0026quot;ifany\u0026quot;) ## ## Hamilton Jay Madison \u0026lt;NA\u0026gt; ## 51 5 14 15 In fact, in the end, I’ll present two network visualizations, using different preparations of the data. For the first, I use the text preprocessing methods that I’ve previously used with this vis technique, which, I think is appropriate for thematically associating documents. For the second, I reduce the amount of text preprocessing, which might in better who-wrote-it, “textual forensics” work. This mirrors how this mini-project developed, and I think two approaches are interesting to contrast.\n Getting started One thing that I like about this four-hour-and-counting project (my one-hour one-off dreams elude me again), is that it makes use of so many of my favorite packages. Let’s load them.\nlibrary(quanteda) # text analysis library(tidyverse) # ggplot for graphing and dplyr for wrangling library(tidygraph) # for relational data manipulation library(igraph) # network graph tools library(ggraph) # the link between ggplot and library(viridis) # for better color palette for points on map library(plotly) # plan to use this in the future, for mouse-over and view full text.  Now we prepare the corpus. We use a bag-of-words approach, where the tokens of each document are counted. This is done using the document feature matrix function or dfm(). I set several preprocessing options to “true”; I think these represent pretty typical preprocessing choices.\nUsing the preprocessed data, we calculate a similarity score. I use the popular cosine similarity in this exercise. The procedure looks at the “direction” of the vector of a document created with each token as a dimension, and compares this to that calculated for other documents. A similarity scores between 0 (low similarity) and 1 (identical token ratios) results for each document pair. Here is the code, which uses functions from the quanteda package:\ncorpus \u0026lt;- corpus(federalist) dfm \u0026lt;- dfm(corpus, remove_numbers = T, remove_punct = T, remove_separators = T, stem = T, remove = stopwords(\u0026quot;english\u0026quot;) ) # Calculate cosine similarity similarity \u0026lt;- textstat_simil(dfm, method = \u0026quot;cosine\u0026quot;, upper = T, diag = T) ## Warning: Arguments upper, diag not used.  Resulting Graph Now the resulting similarity matrix, and treat it as the basis for my network matrix. I simply need to have ones-and-zeros instead of the similarity measure (which takes values between 0 and 1).\nThere are different procedures that I could apply. What I think works well and makes sense for corpus visualizations is to draw an edge (1) when, for a pair of documents, the cosign similarity for the pair is within the top quantile (say .95) of the set of document similarities for either pair member; otherwise don’t draw an edge (0). My rule here is that a connection should be drawn to the top two most similar documents from any document.\nIn this blogpost I’m not going to echo all the code. It is a bit lengthy; it could also stand to be cleaned up a bit. (I haven’t yet implemented the power of the tidygraph package - which I’ve tried elsewhere; it is all that you would hope for!) Perhaps, it is that want I like to keep a few implementation secrets to myself, at least at this point.\nSo I just give you the resulting graph:\n## Warning: Ignoring unknown parameters: colour  Take 2: A revised preparation Now I do the same thing using the alternate preparation. When I used the above preprocessing method, basically the template that I’d used before, I felt disappointed. I expected there to be more “clustering” among the authors; but what seemed to really dominate was topics. It made sense, but I still felt let down.\nThankfully, I took a step back from fiddling with the graphing parameters to review all the code. I came back to the preprocessing code. Hmm. What was being done to alter the raw text? Lots! And there could be the idiosyncratic, tell-tale authorship “signatures” in what was being removed and standardized. So, I modified the preprocessing step to do less to the raw text; below I just comment out the options so the difference is clear. Instead of removing stopwords, we keep them. And we don’t do any stemming. We keep punctuation and numbers too. Then I prep the recalculated similarity scores into an adjacency matrix to allow for a revised network graph.\ncorpus \u0026lt;- corpus(federalist) dfm \u0026lt;- dfm(corpus, # remove_numbers = T, # remove_punct = T, remove_separators = T #, # stem = T, # remove = stopwords(\u0026quot;english\u0026quot;) ) # Calculate cosine similarity similarity \u0026lt;- textstat_simil(dfm, method = \u0026quot;cosine\u0026quot;, upper = T, diag = T) ## Warning: Arguments upper, diag not used. Graphs when cosign similarity contains stop words The adjacency matrix is prepped in the exact same way as before.\n## Warning in summary_character(texts(object), n = n, tolower = tolower, ...): ## verbose argument is defunct ## Warning: Ignoring unknown parameters: colour That’s more like it! I was hoping to observe more separation between authors, and with less preprocessing, that’s exactly what we see. Great!\nAlso in other analyses the disputed texts are in the Madison “zone”, and most scholars that have done textual studies believe the texts were indeed written by Madison, and a few maybe jointly by Madison and Hamilton.\n  Wrapping up Worries The unsatisfying thing about this exercise is that you don’t know which particular word choices and idiosyncratic features drive similarities. But that is the case when using a summary like cosign similarity. You would have to dig in further to look at influential components, I suppose.\nI’m always worried that length of documents drive statistic too. Maybe length of documents contributes to finding cosign similarities. Madison’s works on average are longer than Hamilton’s. Could this be driving things. Maybe.\n## Joining, by = c(\u0026quot;name\u0026quot;, \u0026quot;title\u0026quot;, \u0026quot;venue\u0026quot;, \u0026quot;date\u0026quot;, \u0026quot;author\u0026quot;)`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  The Alternative Perhaps a classic alternative visualization is to show all the cosign similarities with a heat map. Then we aren’t deleting so much information as I do with the network graph — in the network graph we only know if a document is among the top two most similar, or not.\nI make it here, sorting by author, but I don’t really clean it up, maybe to put my technique in the best light, maybe because I’ve really spent too much time on this. But, I think there is something charming and likable about the network approach compared to the heat map. Something friendlier. And that’s where I’ll leave things for now.\n## Warning: Arguments upper, diag not used.   ","date":1529107200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529107200,"objectID":"2df582d89f9679bab768c6d6b3bd797c","permalink":"/post/federalist-papers/","publishdate":"2018-06-16T00:00:00Z","relpermalink":"/post/federalist-papers/","section":"post","summary":"Every couple of weeks I like to explore data that’s brand new to me. I anticipate a one-hour, one-off project. Usually this turns out to be a beautiful lie, and the projects chew up much more time. Still, this enticing time-line is pulling me into new projects from time to time.\nEarlier this week, I heard about the dispute of authorship of some of the Federalist papers.\n\u0026quot;Hamilton wrote the other 51!","tags":[],"title":"Federalist Papers","type":"post"},{"authors":null,"categories":[],"content":" In a previous post, I’ve looked at walking through the calculation of variance and standard deviation, visualizing each step. This post is dedicated to the visualization of another statistic: covariance.\nCovariance is a measure of the joint variability of two random variables.\nLet’s have a look at the sample covariance equation over all:\n\\(cov(x,y) = \\frac{\\sum_{i=1}^n (x_i-\\overline{x})(y_i-\\overline{y})}{n-1}\\)\nAnd now lets apply the equation to the following case:\nscatter \u0026lt;- ggplot(df) + theme_classic() + coord_fixed() + aes(x, y, fill = rectangle \u0026gt; 0) + geom_point() scatter Ready? Okay, now let’s walk through the calculation; there are 7 small steps:\nStep 1: find the mean of x: \\(\\overline{x}\\) scatter + geom_rug(aes(y = NULL)) + geom_vline(xintercept = mean(x), lty = \u0026quot;dashed\u0026quot;)    Step 2: find the mean of y \\(\\overline{y}\\) scatter_means \u0026lt;- scatter + geom_vline(xintercept = mean(x), lty = \u0026quot;dashed\u0026quot;) + geom_hline(yintercept = mean(y), lty = \u0026quot;dashed\u0026quot;) scatter_means + geom_rug(aes(x = NULL))    Step 3: calculate difference between x and mean of x \\(x_i-\\overline{x}\\) scatter_means + geom_segment(mapping = aes(col = x \u0026gt; mean(x)), xend = mean(x), yend = y, arrow = arrow(ends = \u0026quot;first\u0026quot;, length = unit(0.1, \u0026quot;inches\u0026quot;)))    Step 4: calculate difference between y and mean of y \\(y_i-\\overline{y}\\) last_plot() + geom_segment(mapping = aes(col = y \u0026gt; mean(y)), xend = x, yend = mean(y), arrow = arrow(ends = \u0026quot;first\u0026quot;, length = unit(0.1, \u0026quot;inches\u0026quot;)))   Step 5: multiply these differences (observation-wise) \\((x_i-\\overline{x})(y_i-\\overline{y})\\) last_plot() + geom_rect(xmin = mean(x), ymin = mean(y), ymax = y, xmax = x, alpha = .2 )    Step 6: Add these areas \\(\\sum_1^n (x_i-\\overline{x})(y_i-\\overline{y})\\) ggplot(df %\u0026gt;% arrange(x)) + aes(y = rectangle, x = as.factor(1:nrow(df)), fill = rectangle\u0026gt;0) + geom_col(alpha = .2) + theme_bw()   Step 7: Divide through by number of observations minus 1 (the result will a bit larger in magnitude than the average) \\(cov(x,y) = \\frac{\\sum_{i=1}^n (x_i-\\overline{x})(y_i-\\overline{y})}{n-1}\\) last_plot() + geom_col(aes(y = sum(rectangle)/(nrow(df)-1)), alpha = .2, fill = \u0026quot;grey\u0026quot;) + geom_hline(yintercept = sum(df$rectangle)/(nrow(df)-1), lty = \u0026quot;dotted\u0026quot;) That’s it.\nNow we can compare this visualized result to what we would get if we simply trust the R covariance function to calculate this for us.\nsum(df$rectangle)/(nrow(df)-1) ## [1] 0.4766744 cov(x,y) # Calculation for **sample** covariance ## [1] 0.4766744 Great. It’s a match!\n  Discussion question What would the units of unadjusted covariance be for the covariance between life expectancy in years and per capita gdp in dollars?\nNote: The normalized version of covariance is Pearson’s correlation coefficient.\n References R Core Team (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.\n ","date":1528934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528934400,"objectID":"f11f459eafb85e0b4ba0f6ac9418cbb0","permalink":"/post/geometric-covariance/","publishdate":"2018-06-14T00:00:00Z","relpermalink":"/post/geometric-covariance/","section":"post","summary":"In a previous post, I’ve looked at walking through the calculation of variance and standard deviation, visualizing each step. This post is dedicated to the visualization of another statistic: covariance.\nCovariance is a measure of the joint variability of two random variables.\nLet’s have a look at the sample covariance equation over all:\n\\(cov(x,y) = \\frac{\\sum_{i=1}^n (x_i-\\overline{x})(y_i-\\overline{y})}{n-1}\\)\nAnd now lets apply the equation to the following case:\nscatter \u0026lt;- ggplot(df) + theme_classic() + coord_fixed() + aes(x, y, fill = rectangle \u0026gt; 0) + geom_point() scatter Ready?","tags":[],"title":"Covariance -- A Visual Walk Through","type":"post"},{"authors":null,"categories":[],"content":" Sir Francis Galton described the Central Limit Theorem in this way:\nI know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the “Law of Frequency of Error”\nWhy is Sir Francis Galton so excited? Because the Central Limit Theorem (CLT) gives us such clear expectations for random sample means! (A random sample mean is simply the mean of a value for a random sample from a population of interest.) The CLT gives us insight into the question: if we draw a random sample from a population, and calculate the mean for the sample, how close do we expect our sample mean (observed) to be to the true mean (unobserved).\nTo think about this more, we conduct a thought experiment accompanied by a computational experiment.\nThe population In the experiment we’ll be omniscient, but then we’ll think about the perspective of an observer whose knowledge is limited.\nAs the omniscience being, we will know all about the population and will even create it. Let’s do that now.\nx_pop \u0026lt;- runif(10000) # create a popluation with measurable x The population observations has a feature, x, which we measure for all the units of the population. We can look at the distribution of the variable, and its mean, using the functions hist() and mean().\nhist(x_pop) # what does the distribution look like for x mean(x_pop) # True mean of x (popluation mean) ## [1] 0.4943449  A sample Now let’s think about the researcher who is not omniscient, but will only observe a random sample from the population. We use the function sample to take a random sample of the population. The population has 100 members, and we sample 30 with the function sample(), which takes a random sample.\nx_sample \u0026lt;- sample(x = x_pop, size = 30) mean(x_sample) # sample mean ## [1] 0.5194902 Interesting. The sample mean for this one sample is not so far from the population mean.\n Samples that might have been In general, how far would we expect the sample mean to be from the population mean? What might we have observed if different samples had been taken?\nWe are going to use a “for-loop” consider all the samples that might have been taken.\nA “for-loop” simply repeats a routine for you, sometimes with different inputs for each time through the routine.\nHere is a simple for loop. The loop performs a task (printing “Number:” and the time through the loop) ten times, where the input i changes from 1 to 10.\nfor(i in 1:10){ print(paste(\u0026quot;Number:\u0026quot;, i)) } ## [1] \u0026quot;Number: 1\u0026quot; ## [1] \u0026quot;Number: 2\u0026quot; ## [1] \u0026quot;Number: 3\u0026quot; ## [1] \u0026quot;Number: 4\u0026quot; ## [1] \u0026quot;Number: 5\u0026quot; ## [1] \u0026quot;Number: 6\u0026quot; ## [1] \u0026quot;Number: 7\u0026quot; ## [1] \u0026quot;Number: 8\u0026quot; ## [1] \u0026quot;Number: 9\u0026quot; ## [1] \u0026quot;Number: 10\u0026quot; Now back our thought experiment about sampling. What are our expectations about the distribution of sampling means that might be observed for a random sample of the same size?\nLet’s repeat the procedure we did above, ten times through. We use the for-loop to do this work for us. We need to use print() in a for-loop to display output.\nfor(i in 1:10){ x_sample \u0026lt;- sample(x_pop, 30) print(mean(x_sample)) # sample mean } ## [1] 0.5005794 ## [1] 0.5171073 ## [1] 0.4646561 ## [1] 0.4730038 ## [1] 0.4200334 ## [1] 0.4308563 ## [1] 0.5472225 ## [1] 0.4747241 ## [1] 0.4381568 ## [1] 0.4435396 Nice! We see 10 different sample means that we might have observed in the case of different random draws of 30 cases! We begin to see a pattern. The means are nearby, sometimes lower, sometimes higher to our true value.\n Expectations in the limit By doing this experiment more times, we can even more precisely illustrate our expectations about where possible sampling means fall in relation to the true population mean.\n# Expectations for repeated sampling all_samples_means \u0026lt;- c() # this vector is an empty container where we will save means for(i in 1:10000){ x_sample \u0026lt;- sample(x = x_pop, size = 30) all_samples_means[i] \u0026lt;- mean(x_sample) # save the mean for trial i in position i in vector } Now we have a vector of sample means that we could have observed, if different samples were realized.\nLets use the hist() function to have a quick look at the distribution of the data.\nhist(all_samples_means) You will notice that the might-have-been-observed sample means are centered around the true population mean. And that these sample means are normally distributed. This regularity is the observation of the Central Limit Theorem that has Galton (and now you?) so excited.\n Going further: Other population distributions What if the distribution in our population looked different? What would the expectation for the distribution of sample means look like?\nx_pop \u0026lt;- c(rnorm(400), runif(600)) hist(x_pop) x_pop \u0026lt;- c(rnorm(50,mean = 10), rnorm(50, mean = 5)) hist(x_pop) Galton’s quotation continues: Whenever a large sample of chaotic elements are taken in hand and marshalled in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along.\nWhat is the regularity that you find in the results, with different populations as the starting point?\n ","date":1528329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528329600,"objectID":"7a3bac56c57e6ecbae247f8c0c6ffff8","permalink":"/post/central-limit-theorem/","publishdate":"2018-06-07T00:00:00Z","relpermalink":"/post/central-limit-theorem/","section":"post","summary":"Sir Francis Galton described the Central Limit Theorem in this way:\nI know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the “Law of Frequency of Error”\nWhy is Sir Francis Galton so excited? Because the Central Limit Theorem (CLT) gives us such clear expectations for random sample means! (A random sample mean is simply the mean of a value for a random sample from a population of interest.","tags":[],"title":"Central Limit Theorem Demonstration","type":"post"},{"authors":null,"categories":[],"content":" This post goes back and forth between computing statistics for a single variable and visualizing these values. I’m using a subset of the gapminder dataset — European countries in 2007, and focusing on the Life Expectancy variable.\nThis post is developed from lecture slides for my great students in my intro to data science and statistics course at TU Dresden.\nI’m most excited about the walk-through of the variance calculation. In preparing to lecture on univariate stats, I came across a nice visualization of variance explained but couldn’t find a visualization just focused on explaining the variance itself. So I made one with ggplot(). You’ll find it in the second half of the post! (I’m going back and forth about whether to include to the code for the plots or not.)\nSo first we load the tidyverse, which will load ggplot for data visualization and dplyr which we’ll use for data manipulation.\nlibrary(tidyverse) library(gapminder) gapminder_2007_europe \u0026lt;- gapminder %\u0026gt;% filter(year == 2007) %\u0026gt;% filter(continent == \u0026quot;Europe\u0026quot;) Now I prep a very basic plot. It several lines of code, but most of it is styling; you can actually get away with just the first three lines.\nN You can count the number of observations in your data set using the function, nrow().\nnrow(gapminder_2007_europe) ## [1] 30 Below is one way to tally the number of observations that are missing (NA) in the variable.\n# how many missing? sum(is.na(gapminder_2007_europe$lifeExp)) ## [1] 0 # how many not missing? sum(!is.na(gapminder_2007_europe$lifeExp)) ## [1] 30  Measures of Central Tendancy Mean The mean is the sum of the values of a variable divided by the number of values. It is a measure of “central tendancy.”\nThe mean can also be thought of as a balancing point. It the point that, if the folcrum of a balance, with weights of of equal weight at each of the values of the observations, would result in a balanced scale.\n\\[\\mu = \\frac{\\sum_{i=1}^{n}x_i}{n}\\] In R you can use the mean function to compute this value.\nmean(gapminder_2007_europe$lifeExp) ## [1] 77.6486 In the following plot, the mean is plotted in red.\n Measures of Central Tendency: Median Another measure of centrality is the median. The median is the value that has the central position when all the values of are arranged in order.\nIn R you can comute the median with the function, median().\nmedian(gapminder_2007_europe$lifeExp) ## [1] 78.6085 In the following plot, the median is shown in blue.   Measures of spread Range The function range() will return the min and max of a variable.\nrange(gapminder_2007_europe$lifeExp) ## [1] 71.777 81.757 I add the min and max on the plot.\n  Measures of spread/distribution Range + Median + Innerquartile Range To find the value at the lower and upper quartile of the data, I usually use quantile, which the argument probs set equal to .25 and .75.\nquantile(gapminder_2007_europe$lifeExp, probs = c(.25,.75)) ## 25% 75% ## 75.02975 79.81225   Communicating spread/distribution  Boxplot  Measure of spread Variance \\(\\sigma^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}\\)\nYou can also use summary to calculate all of these things at once.\nsummary(gapminder_2007_europe$lifeExp) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 71.78 75.03 78.61 77.65 79.81 81.76 Summary can be used on an entire dataframe too.\nsummary(gapminder_2007_europe) ## country continent year lifeExp ## Albania : 1 Africa : 0 Min. :2007 Min. :71.78 ## Austria : 1 Americas: 0 1st Qu.:2007 1st Qu.:75.03 ## Belgium : 1 Asia : 0 Median :2007 Median :78.61 ## Bosnia and Herzegovina: 1 Europe :30 Mean :2007 Mean :77.65 ## Bulgaria : 1 Oceania : 0 3rd Qu.:2007 3rd Qu.:79.81 ## Croatia : 1 Max. :2007 Max. :81.76 ## (Other) :24 ## pop gdpPercap ## Min. : 301931 Min. : 5937 ## 1st Qu.: 4780560 1st Qu.:14812 ## Median : 9493598 Median :28054 ## Mean :19536618 Mean :25054 ## 3rd Qu.:20849695 3rd Qu.:33818 ## Max. :82400996 Max. :49357 ##    Variance: Visual walk through Now let’s walk through this formula piece by piece. First we need the mean.\nVariance: \\(\\sigma^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}\\)\nThen we do an operation for each of our observations. Take the difference of the value for the observation and square it. Let’s just do that for one of our observations. Romania sounds good. We plot it in green.\nThe difference between and the life expectancy for Romania is represented by a distance in our graph. This difference is shown as a green line.\nVariance: \\(\\sigma^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}\\) Calculation for Romania: \\((x_{Romania} - \\mu)\\) Variance: \\(\\sigma^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}\\)\nThen we need to square that difference. This resultant value is equal to the area of a square that has the length of the distance between of our life expectancy for Romania and the mean life expectancy. It is a transparent green square in the plot.\nCalculation for Romania: \\((x_{Romania} - \\mu)^2\\) Variance: \\(\\sigma^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}\\)\nNow we need to do the same for all of our observations. Not just Romania, but also Germany, Italy, Sweden, France, and so on.\nThe mean for these areas is the variance.\nWe sum up all of the resulting areas that are shown in the plot (note that these are overlapping):\n\\(\\sum_{i=1}^{n}(x_i - \\mu)^2\\)\nAnd then we divide through by the number of observations (e.g. the total number of squares) which gives us our aim - the variance, represented as \\(\\sigma^2\\):\n\\(\\sigma^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}\\)\nI’m plotting the variance as a square where the area is the variance, and a corner of this square (in purple) happens to be at the mean value. The square’s area is the average area for all the squares plotted above, and is equal to the variance.\n Standard Deviation: Now, calculating the standard deviation is straightforward. Take the square root of the variance.\nStandard Deviation: \\(\\sigma = \\sqrt\\frac{\\sum_{i=1}^{n}(x_i - \\mu)^2} {n}\\)\nThe standard deviation on the plot can be represented as simply the length of the edge of the square whose area is the variance.\nQuiz question:\nWhat are the units of each?\n ","date":1528243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528243200,"objectID":"5d008d5b1844415e262c9b3c54e0ecdb","permalink":"/post/univariate-statistics-visualizing-variance/","publishdate":"2018-06-06T00:00:00Z","relpermalink":"/post/univariate-statistics-visualizing-variance/","section":"post","summary":"This post goes back and forth between computing statistics for a single variable and visualizing these values. I’m using a subset of the gapminder dataset — European countries in 2007, and focusing on the Life Expectancy variable.\nThis post is developed from lecture slides for my great students in my intro to data science and statistics course at TU Dresden.\nI’m most excited about the walk-through of the variance calculation.","tags":[],"title":"From N to Standard Deviation","type":"post"},{"authors":null,"categories":[],"content":" Here is the post is about how to implement a layered presentation of a graphics. Matthew Blackwell tweeted about the concept earlier this year, which seemed to reasonated with a lot of folks - to the tune of \u0026gt; 1500 likes! Practical, pedegogical stuff tends to get people excited. True for me too.\nMy best tip on how to give better quantitative presentations is to (a) use more plots and (b) build up your plots on multiple overlays, as in:\n- Just x-axis (explain it)\n- Add y-axis (explain it)\n- Add 1 data point (explain it)\n- Plot the rest of the data (explain it)\n\u0026mdash; Matt Blackwell (@matt_blackwell) April 30, 2018  Anyway, this to me sounded a lot like the ggplot “layered grammar of graphics” ideology, and I retweeted this thought which resonated with a lot fewer people!\nGood ideas! Very #ggplot. A layered presentation of graphics. https://t.co/M4tRB9bGS5\n\u0026mdash; Gina Reynolds (@EvaMaeRey) May 1, 2018  So then I set out to write an implementation with ggplot. I actually wrote a couple of implementations and what follows is my favorite, which is demonstrated using a year of data from the gapminder data package.\nlibrary(tidyverse) library(gapminder) data = gapminder %\u0026gt;% filter(continent == \u0026quot;Americas\u0026quot; \u0026amp; year == 2002) Okay, the data is ready and packages loaded. Now for plotting:\nStep 1: Just x-axis (explain it) ggplot(data) + # declare the data you want to use aes(x = gdpPercap) + # declare the aesthetic mapping for x theme_bw(base_size = 20) # we want label sizes to be a bit bigger  Step 2: Just y-axis (explain it) last_plot() + # we just add to the last plot aes(y = lifeExp) # add the aesthetic mapping for y  Step 3: Add 1 data point (explain it) last_plot() + geom_point() + # declare the geom to add aes(alpha = country == \u0026quot;Chile\u0026quot;) + # alpha is transparency aesthetic scale_alpha_discrete(guide = FALSE, range = c(0,1))  ## Warning: Using alpha for a discrete variable is not advised. Above, we use an extreme range, so Chile is fully opaque and the rest of the points are fully transparent. So the point for Chile is the only point that will appear.\n Step 4: Plot the rest of the data (explain it) Now we overwrite the alpha levels so that all are observations (countries) are represented with a fully opaque point.\nlast_plot() + scale_alpha_discrete(guide = FALSE, range = c(1,1))  ## Warning: Using alpha for a discrete variable is not advised. ## Scale for \u0026#39;alpha\u0026#39; is already present. Adding another scale for \u0026#39;alpha\u0026#39;, which ## will replace the existing scale. Done!\nSo, a (big) bonus of this exercise was discovering that you can use aes() to set global aesthetics outside of the ggplot() function. This is not the usual way that ggplot is taught (see the RStudio Cheatsheats for example) but I really like it, because using it gives you even more of that “building-up-the-plot”, step-by-step, layered feel that makes ggplot so attractive!\n ","date":1526860800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526860800,"objectID":"9b6390a07bfb67b48406f7fa85956c47","permalink":"/post/layered-presentation-of-graphics-with-aes-in-ggplot2/","publishdate":"2018-05-21T00:00:00Z","relpermalink":"/post/layered-presentation-of-graphics-with-aes-in-ggplot2/","section":"post","summary":"Here is the post is about how to implement a layered presentation of a graphics. Matthew Blackwell tweeted about the concept earlier this year, which seemed to reasonated with a lot of folks - to the tune of \u0026gt; 1500 likes! Practical, pedegogical stuff tends to get people excited. True for me too.\nMy best tip on how to give better quantitative presentations is to (a) use more plots and (b) build up your plots on multiple overlays, as in:","tags":[],"title":"Layered Presentation of Graphics with +aes() in ggplot2","type":"post"},{"authors":null,"categories":[],"content":" This is my first post just a test of blogdown.\nIt seems to be working fine also with this R code.\nx = 5 x^2 ## [1] 25 And this Python code:\nprint(\u0026quot;hello\u0026quot; + \u0026quot;there\u0026quot;) ## hellothere Let’s check out how a graph looks too:\nlibrary(ggplot2) ggplot(mtcars) + aes(mpg, disp, col = as.factor(cyl)) + geom_point() Looks good. I’m very satisfied. Hooray for blogdown!\n","date":1526688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526688000,"objectID":"52839b1a63adb29bc2bbc5922c69a103","permalink":"/post/first-blogdown-post/","publishdate":"2018-05-19T00:00:00Z","relpermalink":"/post/first-blogdown-post/","section":"post","summary":"This is my first post just a test of blogdown.\nIt seems to be working fine also with this R code.\nx = 5 x^2 ## [1] 25 And this Python code:\nprint(\u0026quot;hello\u0026quot; + \u0026quot;there\u0026quot;) ## hellothere Let’s check out how a graph looks too:\nlibrary(ggplot2) ggplot(mtcars) + aes(mpg, disp, col = as.factor(cyl)) + geom_point() Looks good. I’m very satisfied. Hooray for blogdown!","tags":[],"title":"First Blogdown blogpost","type":"post"},{"authors":[],"categories":null,"content":"","date":1516280400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516280400,"objectID":"c93950f0d482c95a6315afa222877bf9","permalink":"/talk/leiden/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/leiden/","section":"talk","summary":"Randomization Inference in International Relations Research: Generating Synthetic International Institutions for Inference and Model Robustness Testing.","tags":[],"title":"RI for IR","type":"talk"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"Helping people make sense of code by presenting code and output side-by-side and step-by-step","tags":["flipbooks"],"title":"Flipbooks","type":"project"},{"authors":null,"categories":null,"content":"This section under construction. But here is content to give you an idea of some of my work and its reception.\nA brilliant visualisation of how the Kolmogorov-Smirnoff test works. Made by @EvaMaeReypic.twitter.com/XtDHZc8OZT\n\u0026mdash; Lionel Page (@page_eco) October 10, 2018 \nSome ideas about how to present diff-in-diff data using base #ggplot2 and #rstats. \u0026quot;Four essential plots.\u0026quot;? Note: treatment here is randomly generated. No treatment effect here! 🤓😎 pic.twitter.com/b9fzDLrA3g\n\u0026mdash; Gina Reynolds (@EvaMaeRey) January 31, 2019 \nBrilliant animation demonstrating how bootstrap works.\nNew samples are created with points from the same initial sample.\nYou run a new estimation each time. 👉 You get a variance for your estimator.\nby @EvaMaeRey pic.twitter.com/yNSrHHpUTA\n\u0026mdash; Lionel Page (@page_eco) June 26, 2019 \nA minimal Galton Board build \\w #rstats #gganimate #tidyr #dplyr! https://t.co/TRUdSpPdf0 Step-by-step presentation in #xaringan + reveal functions. 😊😊😊 pic.twitter.com/RlisDRQgD8\n\u0026mdash; Gina Reynolds (@EvaMaeRey) July 1, 2019 \n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An original collection of statistical visualizations to clarify concepts","tags":["Stats Viz"],"title":"Stats Viz","type":"project"},{"authors":["Evangeline Reynolds","Matthew Winters"],"categories":null,"content":"","date":1456790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456790400,"objectID":"a54a8ebc8d55a971a24c9a5192c917d3","permalink":"/publication/brazil-compliance/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/brazil-compliance/","section":"publication","summary":"International legal scholars have identified and argued for and against new forms of non-consent-based international law. We study variation in Brazilian public opinion about adherence to international law created in three different ways: through a consent-based multilateral treaty, by the U.N. Security Council with the participation of Brazil, and by the U.N. Security Council without the participation of Brazil. Information that Brazil has participated in creating the international legal obligation through a multilateral treaty or membership on the Security Council yields levels of support for adherence to the legal obligation that are similar to those found when the origins of the legal obligation are generic. Information that the international legal obligation was created without Brazil’s participation, on the other hand, results in reduced support for compliance. This difference, which is particularly concentrated among highly educated respondents, is not driven by reduced concerns about reputational consequences or sanctions. Our results suggest that the increased use of non-consent-based forms of international law might be challenged by a lack of public support for compliance.","tags":["Source Themes"],"title":"Attitudes toward Consent-Based and Non-Consent-Based International Law in a Regional Power Context","type":"publication"},{"authors":["Evangeline Reynolds","Matthew Winters"],"categories":null,"content":"","date":1456790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456790400,"objectID":"b34959cd0a6c687eb2925f4d0da625e8","permalink":"/publication/foreign-aid/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/foreign-aid/","section":"publication","summary":"A number of recent studies have found that temporary members of the United Nations Security Council (UNSC) experience increased foreign aid inflows. We use a constrained permutations approach to replicate analyses found in Vreeland and Dreher (2014). Permuting the timing of country membership on the Security Council, we create placebo UNSC membership histories which plausibly could have been observed. We use these placebos to construct a reference distribution for the null hypothesis that there is no relationship between UNSC membership and foreign aid flows and then observe whether or not the observed test statistic for the correlation found in the real-world data is in the tails of this distribution. In other contexts, such empirically based hypothesis tests have revealed a high false-positive rate for traditional, model-based time-series cross-sectional inference. Given the controversial nature of studies about increased aid flows as secondary benefits of UNSC membership, it is valuable to subject such analyses to additional scrutiny. Our reanalysis largely validates existing findings.","tags":["Source Themes"],"title":"Foreign aid funnel? A placebo-based assessment of aid flows to non-permanent United Nations Security Council members","type":"publication"},{"authors":null,"categories":["R"],"content":" R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932  Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA )  Figure 1: A fancy pie chart.   ","date":1437703994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437703994,"objectID":"10065deaa3098b0da91b78b48d0efc71","permalink":"/post/2015-07-23-r-rmarkdown/","publishdate":"2015-07-23T21:13:14-05:00","relpermalink":"/post/2015-07-23-r-rmarkdown/","section":"post","summary":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"}]